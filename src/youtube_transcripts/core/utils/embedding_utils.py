# src/complexity/arangodb/embedding_utils.py
import os
import hashlib
import numpy as np
from typing import List, Dict, Any, Optional, Union
import sys
import time
from loguru import logger

# Import HuggingFace for BAAI/bge model
try:
    import torch
    from transformers import AutoTokenizer, AutoModel
    has_transformers = True
    logger.info("Transformers library is available for embeddings")
except ImportError:
    has_transformers = False
    logger.warning("Transformers library not available, will use fallback embedding method")

# Import config
try:
    from arangodb.core import EMBEDDING_MODEL, EMBEDDING_DIMENSIONS
except ImportError:
    logger.warning("Failed to import config, using default embedding settings")
    EMBEDDING_MODEL = "BAAI/bge-large-en-v1.5"
    EMBEDDING_DIMENSIONS = 1024

# Initialize BAAI/bge model if available
_model = None
_tokenizer = None

def _initialize_model():
    """Initialize the BAAI/bge embedding model and tokenizer."""
    global _model, _tokenizer
    
    if not has_transformers:
        logger.warning("Transformers library not available, cannot initialize model")
        return False
    
    try:
        logger.info(f"Initializing embedding model: {EMBEDDING_MODEL}")
        _tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)
        _model = AutoModel.from_pretrained(EMBEDDING_MODEL)
        
        # Move model to GPU if available
        if torch.cuda.is_available():
            _model = _model.to("cuda")
            logger.info("Model loaded on GPU")
        else:
            logger.info("Model loaded on CPU")
        
        return True
    except Exception as e:
        logger.error(f"Error initializing embedding model: {e}")
        return False

def get_embedding(text: str, model: str = None) -> Optional[List[float]]:
    """
    Get an embedding vector for a text string using BAAI/bge model.
    
    Args:
        text: The text to embed
        model: Optional model name (defaults to config value)
        
    Returns:
        List of embedding values or None if embedding failed
    """
    global _model, _tokenizer
    
    # Use default model if not specified
    model = model or EMBEDDING_MODEL
    
    logger.info(f"Generating embedding for text: {text[:50]}...")
    
    # We only use BGE model - no OpenAI
    if has_transformers:
        # Initialize model if not done yet
        if _model is None or _tokenizer is None:
            success = _initialize_model()
            if not success:
                return _fallback_embedding(text)
        
        try:
            # Prepare inputs
            encoded_input = _tokenizer(text, padding=True, truncation=True, 
                                      return_tensors='pt', max_length=512)
            
            # Move inputs to GPU if available
            if torch.cuda.is_available():
                encoded_input = {k: v.to("cuda") for k, v in encoded_input.items()}
            
            # Compute embedding
            with torch.no_grad():
                model_output = _model(**encoded_input)
                embedding = model_output.last_hidden_state[:, 0, :].cpu().numpy()[0]
            
            # Normalize embedding
            norm = np.linalg.norm(embedding)
            if norm > 0:
                embedding = embedding / norm
            
            return embedding.tolist()
        
        except Exception as e:
            logger.error(f"Error generating embedding with {EMBEDDING_MODEL}: {e}")
            # Fall back to hash-based method
            return _fallback_embedding(text)
    
    # Use fallback method if transformers not available
    return _fallback_embedding(text)

def _fallback_embedding(text: str) -> List[float]:
    """
    Generate a deterministic fallback embedding using text hash.
    This is only used when the primary embedding method fails.
    
    Args:
        text: The text to embed
        
    Returns:
        List of embedding values
    """
    logger.warning("Using fallback embedding method (hash-based)")
    
    # Use text hash as seed
    text_hash = hashlib.md5(text.encode()).hexdigest()
    seed = int(text_hash, 16) % (2**32)
    
    # Generate deterministic random vector
    np.random.seed(seed)
    embedding = np.random.normal(0, 1, EMBEDDING_DIMENSIONS)
    
    # Normalize the vector
    norm = np.linalg.norm(embedding)
    if norm > 0:
        embedding = embedding / norm
    
    return embedding.tolist()

def get_EmbedderModel():
    """
    Return the embedding model information.
    
    Returns:
        Dict with model name and dimensions
    """
    return {
        "model": EMBEDDING_MODEL,
        "dimensions": EMBEDDING_DIMENSIONS
    }

def cosine_similarity(vector1: List[float], vector2: List[float]) -> float:
    """
    Calculate cosine similarity between two vectors.
    
    Args:
        vector1: First embedding vector
        vector2: Second embedding vector
        
    Returns:
        Cosine similarity score between 0 and 1
    """
    a = np.array(vector1)
    b = np.array(vector2)
    
    # Calculate cosine similarity
    dot_product = np.dot(a, b)
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    
    # Protect against division by zero
    if norm_a == 0 or norm_b == 0:
        return 0.0
    
    similarity = dot_product / (norm_a * norm_b)
    return float(similarity)
    
# Add alias for backward compatibility
calculate_cosine_similarity = cosine_similarity

def validate_embedding_utils():
    """
    Validate the embedding_utils functions using proper verification.
    """
    # Import ValidationTracker if available
    try:
        from arangodb.utils.validation_tracker import ValidationTracker
        validator = ValidationTracker("Embedding Utils Module")
    except ImportError:
        # Define a simple tracking system
        class SimpleValidator:
            def __init__(self, module_name):
                self.module_name = module_name
                self.failures = []
                self.total_tests = 0
                print(f"Validation for {module_name}")
                
            def check(self, test_name, expected, actual, description=None):
                self.total_tests += 1
                if expected == actual:
                    print(f"✅ PASS: {test_name}")
                    return True
                else:
                    self.failures.append({
                        "test_name": test_name,
                        "expected": expected,
                        "actual": actual,
                        "description": description
                    })
                    print(f"❌ FAIL: {test_name}")
                    print(f"  Expected: {expected}")
                    print(f"  Actual: {actual}")
                    if description:
                        print(f"  Description: {description}")
                    return False
                    
            def report_and_exit(self):
                failed_count = len(self.failures)
                if failed_count > 0:
                    print(f"\n❌ VALIDATION FAILED - {failed_count} of {self.total_tests} tests failed:")
                    for failure in self.failures:
                        print(f"  - {failure['test_name']}")
                    sys.exit(1)
                else:
                    print(f"\n✅ VALIDATION PASSED - All {self.total_tests} tests produced expected results")
                    sys.exit(0)
        
        validator = SimpleValidator("Embedding Utils Module")
    
    # Test 1: Get embedding model info
    model_info = get_EmbedderModel()
    validator.check(
        "get_EmbedderModel returns model info",
        expected=True,
        actual=isinstance(model_info, dict) and "model" in model_info and "dimensions" in model_info,
        description="Check model info contains required fields"
    )
    
    # Test 2: Fallback embedding generation
    text_input = "This is a test text for embedding"
    fallback_embedding = _fallback_embedding(text_input)
    validator.check(
        "fallback_embedding generates vector of correct dimension",
        expected=EMBEDDING_DIMENSIONS,
        actual=len(fallback_embedding),
        description="Check fallback embedding has correct dimensions"
    )
    
    # Test 3: Fallback embedding normalization
    norm = np.linalg.norm(np.array(fallback_embedding))
    validator.check(
        "fallback_embedding produces normalized vector",
        expected=True,
        actual=abs(norm - 1.0) < 1e-5,
        description="Check fallback embedding is normalized (norm ≈ 1.0)"
    )
    
    # Test 4: Fallback embedding determinism
    fallback_embedding2 = _fallback_embedding(text_input)
    validator.check(
        "fallback_embedding is deterministic",
        expected=True,
        actual=fallback_embedding == fallback_embedding2,
        description="Check fallback embedding produces same vector for same input"
    )
    
    # Test 5: Different inputs produce different embeddings
    text_input2 = "This is a different text for embedding"
    fallback_embedding3 = _fallback_embedding(text_input2)
    validator.check(
        "Different inputs produce different embeddings",
        expected=True,
        actual=fallback_embedding != fallback_embedding3,
        description="Check different inputs produce different embeddings"
    )
    
    # Test 6: Get embedding function works (even if it uses fallback)
    embedding = get_embedding("Test embedding generation")
    validator.check(
        "get_embedding returns a vector",
        expected=True,
        actual=isinstance(embedding, list) and len(embedding) > 0,
        description="Check get_embedding returns a non-empty vector"
    )
    
    # Test 7: Cosine similarity calculation
    vector1 = [1.0, 0.0, 0.0]
    vector2 = [1.0, 0.0, 0.0]
    vector3 = [0.0, 1.0, 0.0]
    vector4 = [-1.0, 0.0, 0.0]
    
    similarity_identical = cosine_similarity(vector1, vector2)
    validator.check(
        "cosine_similarity of identical vectors = 1.0",
        expected=1.0,
        actual=similarity_identical,
        description="Check cosine similarity of identical vectors = 1.0"
    )
    
    similarity_orthogonal = cosine_similarity(vector1, vector3)
    validator.check(
        "cosine_similarity of orthogonal vectors = 0.0",
        expected=0.0,
        actual=similarity_orthogonal,
        description="Check cosine similarity of orthogonal vectors = 0.0"
    )
    
    similarity_opposite = cosine_similarity(vector1, vector4)
    validator.check(
        "cosine_similarity of opposite vectors = -1.0",
        expected=-1.0,
        actual=similarity_opposite,
        description="Check cosine similarity of opposite vectors = -1.0"
    )
    
    # Test 8: Zero vector handling
    zero_vector = [0.0, 0.0, 0.0]
    similarity_with_zero = cosine_similarity(vector1, zero_vector)
    validator.check(
        "cosine_similarity with zero vector = 0.0",
        expected=0.0,
        actual=similarity_with_zero,
        description="Check cosine similarity with zero vector = 0.0"
    )
    
    # Test 9: calculate_cosine_similarity alias
    validator.check(
        "calculate_cosine_similarity is alias of cosine_similarity",
        expected=True,
        actual=calculate_cosine_similarity is cosine_similarity,
        description="Check calculate_cosine_similarity is an alias to cosine_similarity"
    )
    
    # Generate final report
    validator.report_and_exit()

if __name__ == "__main__":
    validate_embedding_utils()