As you know, MCP servers let you build

tools for everything. MCP servers are

one of the three most important

innovations for evolving your

engineering from AI coding to a gentic

coding. With new models like Claude 4

and the brand new Deepseek

R1.1, we have more intelligence to build

than ever before. But the models are no

longer the limiting factor for your

engineering output that forces us to ask

what's limiting us as engineers from

creating more value faster than ever.

It's our abilities to create

capabilities for our agentic coding

tools like clawed code. That brings us

full circle back to MCP servers. In this

video, we're going to understand the

most underutilized capability of MCP

servers. Most engineers stop at tools.

But once you understand this one simple

idea, you'll be able to craft rich MCP

servers that dramatically increase your

engineering velocity as well as your

teams. Resources, tools, and props. And

the tier list in reverse order of

capability, we have resources, tools,

and prompts. Most engineers skip

resources. They go all in on tools and

completely miss out on the highest

leverage primitive of MCP servers,

props. Tool calling is just the

beginning of your MCP server. Let me

show you how to maximize the value of

your MCP servers.

If we type /mptcp, you can see I have

six MCP servers available. We're going

to be operating in the quick data MCP

server. Quick data gives your agent

arbitrary data analysis capabilities

onJSON and CSV files. We all know how

tools work, but let's run a few to

understand the quick data MCP server and

showcase how limited tool calls really

are. If we type /model, we're going to

run this sonnet 4 fast workhorse model

for this. So, right away we have a

problem. I have no idea what I can do

with this MCP server. I have to rely on

some type of documentation. Let's open

up cursor and break open the readme. If

we scroll down here, I have a complete

documented set of all the tools,

resources, and prompts available for

this MCP server. Let's just start with a

couple simple ones. I'll run this load

data set. And now we need to pass in

ajson or a CSV file. I'll go back to

cursor. If I search for e-commerce

orders. You can see we have this simple

JSON list. I'll copy the reference to

this file with command shift R. Then

I'll hop back to cloud code. Paste this

in and have it load. All right. So, as

expected, we have this load data set MCP

server tool. It has the file path and

the data set name e-commerce orders.

This looks great. We'll go ahead and

accept this. And you can see our JSON

response. If we hit control-R, you can

see the entire thing. columns, rows,

data set name. Looks great. So, let's go

ahead and get a data set breakdown. So,

I'll paste. This also accepts the data

set name. So, go ahead, copy and paste

this back in. And now we're just going

to get some basic information about this

data set. We'll of course accept this

tool call. And you can see we have the

shape and key information about this

data set. So, so far this looks great.

Let's run a couple more tools and then

we're going to uplevel everything we're

doing by looking at the most powerful

capability you can add to your MCP

server. Let's run suggest analysis paste

and then I'll just say ecom dot dot dot.

This is going to be autocompleted for us

based on the current context. There it

is. Suggest analysis. Let's see what we

get. So we have a couple of ideas given

to us based on that tool call. Run

command number one. Fire this off. We're

now going to get a segment breakdown by

this product category column. So check

this out. We have product category

segmentation. We can see that

electronics are producing a lot of value

inside of this e-commerce

orders.json file. So looking at this

data from a business strategy

perspective, we could if we wanted to

cut down on sports and home garden

product categories and go all in on

electronics based on this insight. Okay.

So there's one more cool tool I want to

share with you here. If we scroll down

to the bottom, we can execute arbitrary

code. We can have clawed code running on

claude for sonnet execute arbitrary code

for us. So again, we can just come back

in here, paste, we can say ecom dot dot

dot and let's find out the if we look at

the data set here, we have this region

column and we also have order value. So

let's find out the top order value by

region. Find top three region order

value. Yep, let's go ahead and fire that

off. There we go. So you can see here we

have custom code getting written based

on our prompt. We'll hit yes and there

is our executed code response. You can

see here our top three regions by order

volume. We have East Coast, West Coast

and of course Midwest and last place.

Pretty accurate training data set,

right? If we want to reuse that same MCP

tool call, we can hit up and then I'll

say then create a pie chart label by

region value and percent. It's going to

create a pie chart for us. Let's go

ahead and run this. And bam, check this

out. You can see we have East Coast, we

have West Coast, Midwest, and then the

South. We have a great breakdown here.

And this was all just quickly created

and managed with our MCP server for

quick data analytics against JSON and

CSV. So tools are great. We all know

about their capabilities. We can build

out tools for anything and tools for

everything. But tools only scratch the

surface of what you can do with your MCP

server. To unlock the full capabilities

of what you can do, we need to build MCP

server

prompts. So in order to showcase the

capabilities here, we're going to reset

this cloud code instance and really

start from scratch. So let's open up

cloud again. We'll run the same setup.

So you can see here

/mcp/mod, same deal. Sonnet 4. So now

instead of looking through the

documentation, right, we had this readme

that thankfully detailed all of our

tools, resources, and prompts, right?

There's the codebase structure. We'll

take a look at that in a second. Instead

of doing any of this, instead of, you

know, relying on codebase architecture,

codebase structure, we can just use MCP

server prompts to guide the entire

discovery and use of the quick data MCP

server. Let me show you exactly what I

mean. To find all the prompts associated

with this MCP server inside of cloud

code, we can type

/quick- data. So, this is the name of

the MCP server. And here you can see a

ton of autocomplete suggestions with

prompts. So, these are prompts built out

in the MCP server. Now, we're going to

run something really cool, something

very useful that I highly recommend you

set up inside of all your MCP servers.

We're going to list available MCP server

capabilities, including prompts, tools,

and resources. So, this is a prompt

that's going to give us a clear

breakdown of what we can do with this

tool. Okay, check this out. So, Claude

Code, our agentic coding tool, has now

consumed everything that we can do with

this tool. It's now loaded fresh in the

context window, and we have a quick

start flow to get started. So now if we

want to we can just ask cloud code what

exactly these heat components are. Okay.

So I'm just going to say tools list as

bullets. All right. So check this out.

So now we can just you know query our

agent right here are the prompts. Here

are the tools. This is everything that

we saw before. Let's go ahead and

continue firing off these prompts to

really understand what they can do for

us. All right. So if we type slashfind,

you can see we have another prompt. Find

data sources prompts. This is going to

discover available data files in the

current directory and present them as

load options. Now, see how much more

helpful these prompts are than just

having tools hidden somewhere. I'm going

to hit tab. You can see here we have an

argument, the directory path. I'll just

hit dot for that and fire that off. So,

this is going to automatically discover

all

available.json and CSV files for our

quick data MCP server. So, we had a

prompt, also known as an agentic

workflow. do this work for us

automatically. You can see we also have

take note of this this is really

important ready to load with load data

set commands. So with the previous

prompt and this prompt you can see every

prompt we're running we're getting a

suggestion or a for direction or a next

step for what we can do with this MTP

server. So what I'm going to do here is

just type load ecom. So I have a really

tight information dense keyword prompt

literally just two words with the

current context that we have set up

thanks to our prompts and thanks to claw

code running on claude for sonnet I can

be nearly 100% sure that this is going

to run the right tool with the right

information. Okay. And so I'll kick this

off. And notice how I just, you know,

ran through the big three of AI coding.

Context, model, prompt. These never go

away. That's why they're a principle of

AI coding. They're always there whether

you realize it or not. The more you can

look and think from your agents

perspective with the current available

context model and prompt, the more

you'll be able to hand off tons and tons

of engineering work, which in the end

results in your engineering velocity

increase. So check this out. So, we have

the file path here using the full

absolute path. Looks great. And then we

have the data set name. Okay. With just

typing slash, with just working through

a few pre-existing prompts, we're moving

a lot faster than if we were looking

through, you know, the documentation

going back and forth and back and forth.

And that is a really important thing to

call out here, right? We haven't left

the terminal. We haven't left cloud

code. We're focused. We're moving

quickly. And we're operating inside of

this MCP server with minimal

information. Okay. Okay, so we have that

data set loaded. If we scroll back up,

you'll remember here at the top that we

were given a concrete workflow. You can

see uh find data set to discover data

files and then we can run load data set

and then explore data. So let's go ahead

and run that. I'm going to type slash

first. This is our first look MCP

prompt. I'll hit tab and you can see

there the arguments are data set name.

I'll go ahead and just type ecom dot dot

dot and we should get auto completion

there. Okay, there we go. So this prompt

and we're going to take a look at the

individual prompts in a second is

kicking off one or more tools. Okay. So

we'll go ahead and fire that off. And

based on that prompt, right, and based

on the information returned by this

tool, we're getting a nice breakdown of

a sample of this data set size, columns,

sample data. Looks great. You can see

there we actually did get a sample. If

we hit control-R, it broke down, you

know, pieces of our data. So that looks

great. Thanks to the existing context

window that all of these prompts have

been giving our agent, we can just type

something like this. How can we further

explore this

data? Okay, so check this out. From the

existing context window, we have, you

know, tons of ideas of how we can keep

pushing this. And this is really useful

for when you're operating outside of

your MCP server. Obviously, if you're

building your MCP server, you have

access to the actual code and you can

just kind of, you know, have your agent

run through this. But when we're

operating in this, when we're handing

off our MCP server to our team, to our

engineering team, and when we're

exposing our MCP servers to the public,

we want it easy to use. We want it to be

quickly consumable, and we want these

guided workflows. Okay, prompts are

really important because they can return

entire sets of information to your agent

and they can provide next steps. You

keep pushing yourself, you can keep

pushing engineers on your team and

pushing the agent in the right direction

for your domain specific problem set.

Let's go ahead. Let's run another

prompt. So, we can do slashquick data to

see all of our prompts. Let's go ahead

and run the correlation investigation

prompt. So, this is going to find

correlations inside of our data set.

We'll of course type ecom dot dot dot.

And before we run this, let me show you

exactly what these prompts look like

inside of the MCP server. So, we'll open

up cursor and we're just going to search

for that prompt. Notice how I just have

a single method inside of this file. And

since we're here, let's talk about

codebase architecture. This is

important. So, I have the codebase

embedded in its own directory here. And

on top, you can see the three essential

directories for Aenta coding. And you

can see our trees directory for multi-

aent parallel AI coding. Check out the

previous video to see how we parallelize

cloud code into multiple trees to get

work done at the same time. But if we

click into architecture modular and we

take a look at the architecture here,

you can see we have our data there. Then

we have source MCP server and we have

the primitives of the MCP server, right?

Tools, resources, and prompts. If we

open up prompts, we can see our

correlation investigation prompt here.

Inside of the single function, these are

all single function Python files to keep

everything nice and isolated and easily

testable. So, if we hop up to this file,

we can see something really cool. We're

passing in the data set name and then

we're just running arbitrary code, which

is effectively our agentic workflow. So,

you can do anything you want here. The

most important thing is to gather some

type of prompt response and then return

that back to your agent. Right? This is

what's going to get passed right back

into the agent. So you can see we have

lots of detail on the correlation

investigation. A couple of branches of

logic here, a loop, and you can see

we're loading that schema out of our

existing data set. So let's go ahead.

Let's run this. This is going to run a

really great analysis on our data set.

Okay, so we're going to close that.

Let's run this. So, this prompt is

kicking off a tool call. And this is

super important. Inside of your prompts,

you can kick off one or more tool calls.

You can see here how the prompt allows

you to compose sequences of tool calls

very very quickly using a custom slash

command here. Okay, so quick commands to

start. You can see that we're picking

this up automatically. This is getting

returned into the context window. And

now cloud code running on cloud 4 wants

to kick this off for us based on this

prompt. Okay, so of course we'll hit

yes. And you can see there we're getting

some concrete feedback. Okay, we need at

least two numerical columns for

correlation analysis. Okay, so we can go

ahead and kick this off. This is going

to reexpose information back into our

agent. So e-commerce orders cannot run

this. Okay, so our tools are giving us

feedback all guided by our prompt. Let's

go ahead and load some more data. Right,

we can very quickly thanks to our slash

command just run /find and let's go

ahead and find those other data sources

that we have. I'll specify the directory

here dot and this is going to reload all

of our data sources. And so you can see

here we can load these two additional

data sources. Let's go ahead and load

these. So I'm just going to say load

all. So now we're going to get those two

prompts. There's our employee survey

data set and here's our product

performance data set. We'll hit yes.

Yes. Now you have multiple numerical

columns cross data sets for correlation

analysis. Okay. So I'll just say run

analysis on employee

product. Okay. So there it is. There's

that find correlation lookup and you can

see here it's queued up several calls,

several tool calls that we can now kick

off. So this is one way to, you know,

activate this workflow. That's great.

I'm going to hit escape here and I'm

going to reuse the slash command that we

were we were going for. So I'm going to

type slash um correlation investigation

prompt. Looks great. And then I'll pass

in let's use our employee survey. So

I'll paste that in and let's run the

investigation prompt here. This should

kick off a similar workflow. There it

is. So this prompt is exposing the

potential columns that we can correlate.

So I'm going to go ahead. I want to kick

off this first flow that was revealed by

this prompt. Super simple. It's a slash

command that's exposed by our MCP server

prompts that we just pass in one

variable to work with. Okay. So go ahead

and type uh run option one. Okay. So

this should pop up find correlations.

There it is. Let's fire off our

correlations and let's see what we get

here. So check this out. Strong

correlation found. We have satisfaction

score correlated with tenure years. All

right. So if we open up this data set

just to take a look at this, you can see

several columns. So you can see here

tenure year satisfaction score simple

CSV file and this prompt and the tools

called by the prompt found this strong

correlation. Okay, so there's a strong

positive correlation between

satisfaction score and tenure year. That

means employees with high satisfaction

scores tend to have longer tenure. And

so this reveals, you know, not to get

too specific into the weeds of this MCP

server, but this is important because

it's going to immediately reveal to us

that satisfaction and retention are

closely linked. Satisfied employees stay

longer. Not a mind-blowing revelation,

but this could be anything inside of

your data set. Okay, I'm just I'm just

putting together a small concise example

that we can, you know, discuss to

showcase the power of these MCP server

prompts. Okay, do you want to visualize

this with option two? I'll I'll say go

ahead and we'll just continue walking

through create chart. Let's go and fire

that off. We now have an additional

chart set up here. We can open this up.

So, we can copy this file path here. If

we go into HTML preview mode, we can see

this chart generated. If you average

these out over the satisfaction score,

you can see we have a pretty strong

correlation here between tenure and

satisfaction score. So, very powerful

stuff. So, what does this all mean,

right? Why are prompts inside of your

MCP server so important? Right away, by

using this MCP server, we were able to

move a lot faster. If we close Claude

here, reopen it, and we type

/assets, we can get our agent back up

and running with this MCP server very

quickly. Okay, so prompts let you

quickly set up your agent with

everything they need to know to operate

your MCP server. So this is just one

simple way you can use MCP server

prompts. Check this out. And we can look

at this exact prompt, right? This is the

list MCP assets prompt. So check this

out. Look how simple this is. This is

quite literally just returning essential

information about this MCP server in a

custom way to our agent. This prompt

primes both your memory and your agents

memory with everything it needs to know

about your MCP server. Every MCP server

I build out now has some type of prompt

just like this. So now everything is

exposed. We can quickly see and operate

on things in a much faster way. We can

always type /quick dash and start

understanding our data sets. Right? So

you know we then ran our find and we

passed in dot prompts allow us to prime

our agent in powerful ways and run

arbitrary clawed code tools. So inside

of the find data sources prompt again we

can just search this. I've isolated

everything into its own file. This is

another great pattern I highly recommend

you follow. We have our find data

sources prompt which is running

arbitrary code. Principal AI coding

members know this as an ADW AI developer

workflow. That's all these prompts are.

They're endtoend chains of prompts and

code coupled together that end up in a

simple string return value. So after we

scan all the directories, we do

something really powerful. We in

multiple use cases, in multiple

scenarios, we offer the agent

suggestions. This is really really

powerful. Our agent is ready for the

next step, right? It wants this load

data set uh command. So this time

around, you know, this is an agent. It's

powerful. It's got the new cloud for

model. We can just load all data

sets. Okay, so there's three prompts

instead of one, right? We can move a lot

faster thanks to the prompt. Okay, so we

can load. Bam, bam, bam. Now we have all

three data sets loaded. Fantastic. And

now we can run, you know, our data set

first look. If we wanted to, we can

continue down that line that we were

running before. Slash data set first

look. You know, this is just, you know,

two or three of many prompts that we

have here. There's really no limit to

what you can do with your prompts inside

of your MCP server. So, we have prompts,

resources, and tools. Cloud code does

not support resources. If we open up the

example clients and search cloud code,

you can see here that you know cloud

code does not have resource support, but

it has the two that really matter,

prompts and tools. You can also

substitute your resources for just

specific tool calls. I've done that

inside this codebase. You can check that

out if you're interested. But

recentering on the key idea here, why do

we create prompts? Because prompts allow

us to create agentic workflows. They

allow us to compose our tools. Tools are

individual actions. Here's our load data

set tool. And you can see it just does

one thing, right? It takes one action.

It loads the data set into memory. Tools

are individual actions. Prompts are

recipes for repeat solutions. This is

the big difference. Your prompts have

three massive advantages that your tools

don't have. You can with cloud code

reference all of your prompts in a

clean, detailed way very quickly. Okay,

so no more guessing so you can quickly

get up and running with whatever MCP

server you have. So second, your prompts

can compose tools in your MCP server

together. Okay, this is super super

powerful. You saw multiple times here

our prompt was then kicking off

individual tools that exists underneath

the prompts, right? That's why we have

this tier list order of capability

prompts greater than tools greater than

resources. And lastly, a super super

underutilized element of prompts is that

you can guide the experience. At the

end, our agent is saying, "Use data set

first look to explore any data set." So,

our agent through the prompt that was

run here, right? If we scroll up here,

we have find data resources prompt, it

triggered a whole slew of not just a

sequence of tools, but also a guide and

a direction for you, the engineer

operating the tool. And more

importantly, every single day, it's

giving our agent the next steps. Okay,

so load data set, load data set, load

data set. And then we can just, you

know, very quickly, very calmly say

things like this, load all data sets,

and then continue down the line of data

exploration or running whatever other

tools or prompts our MCP server exposes

to us. With prompts, you can build out

high quality MCP servers that do more

than just call tools. Tool calling is

just the beginning. Tools are the

primitives of MCP servers, not the end

state. You want to end up with prompts,

right? Prompts represent end-to-end

developer workflows that are truly

agentic workflows or as I like to call

them, AI developer workflows, right?

They are quite literally doing developer

work that you would do, but it's

powered, of course, by Gen AI. You

really want to be thinking about MCP

servers as a way to solve a domain

specific problem in an automated fashion

with repeat solutions embedded inside of

the prompts. The prompt is what the

tools scale into. This codebase is

linked in the description to give you a

concrete example of how you can use

prompts inside your MCP servers. Come in

here, play with it, cd into this folder

name. I'll probably change this by the

time you see it and you'll be able to

quickly boot up Cloud Code with this

MCP.json file here. If you made it to

the end, like comment to let the YouTube

algorithm know you want more hands-on

engineering information like this. You

know where to find me every Monday. Stay

focused and keep building.